import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
import re
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx  # åç»­node2vecå®éªŒå¯ç”¨ï¼Œå…ˆå¯¼å…¥
import warnings

warnings.filterwarnings('ignore')


# -------------------------- 1. æ–‡æœ¬é¢„å¤„ç†æ¨¡å—ï¼ˆåŸæœ‰å¢å¼ºï¼‰ --------------------------
def preprocess_text(text):
    """
    æ–‡æœ¬é¢„å¤„ç†å‡½æ•°ï¼šå¤„ç†ç¼ºå¤±å€¼ã€å°å†™è½¬æ¢ã€ç§»é™¤ç‰¹æ®Šå­—ç¬¦ã€åˆ†è¯
    :param text: åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²
    :return: åˆ†è¯åçš„è¯æ±‡åˆ—è¡¨
    """
    if pd.isna(text):
        return []
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—ï¼ˆå¯é€‰ï¼Œæ ¹æ®å®éªŒéœ€æ±‚è°ƒæ•´ï¼‰
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    # ç®€å•åˆ†è¯ï¼ˆä¹Ÿå¯æ›¿æ¢ä¸ºnltk.word_tokenizeï¼Œéœ€ä¸‹è½½punktï¼‰
    tokens = text.split()
    # ç§»é™¤åœç”¨è¯ï¼ˆå¯é€‰ï¼Œæå‡è¯å‘é‡è´¨é‡ï¼‰
    stop_words = set(['a', 'an', 'the', 'and', 'or', 'but', 'is', 'are'])
    tokens = [token for token in tokens if token not in stop_words and len(token) > 1]
    return tokens


def load_and_preprocess_data(file_path):
    """
    åŠ è½½å¹¶é¢„å¤„ç†Amazonè¯„è®ºæ•°æ®
    :param file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
    :return: é¢„å¤„ç†åçš„è¯­æ–™åº“ã€æ ‡ç­¾ã€åŸå§‹DataFrame
    """
    # è¯»å–CSVæ–‡ä»¶ï¼ˆé€‚é…Amazonæ•°æ®é›†æ ¼å¼ï¼‰
    df = pd.read_csv(file_path, on_bad_lines='skip')  # è·³è¿‡é”™è¯¯è¡Œ
    # ç¡®ä¿è‡³å°‘æœ‰3åˆ—ï¼ˆæ ‡ç­¾ã€æ ‡é¢˜ã€è¯„è®ºï¼‰
    if df.shape[1] < 3:
        raise ValueError("æ•°æ®é›†éœ€åŒ…å«è‡³å°‘3åˆ—ï¼šæ ‡ç­¾ã€æ ‡é¢˜ã€è¯„è®ºå†…å®¹")

    # åˆå¹¶æ ‡é¢˜å’Œè¯„è®ºå†…å®¹ï¼Œé¿å…ç±»å‹é”™è¯¯
    df['text'] = df.iloc[:, 1].astype(str) + " " + df.iloc[:, 2].astype(str)

    # é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬ç”Ÿæˆè¯­æ–™åº“
    corpus = [preprocess_text(text) for text in df['text']]

    return corpus, df.iloc[:, 0].values, df


# -------------------------- 2. Word2Vecæ¨¡å‹è®­ç»ƒï¼ˆæ”¯æŒä¸¤ç§æ¨¡å¼ï¼‰ --------------------------
def train_word2vec_model(corpus, model_type='cbow'):
    """
    è®­ç»ƒWord2Vecæ¨¡å‹ï¼Œæ”¯æŒCBOWå’ŒSkip-gramä¸¤ç§æ¨¡å¼
    :param corpus: é¢„å¤„ç†åçš„è¯­æ–™åº“
    :param model_type: æ¨¡å‹ç±»å‹ï¼Œ'cbow'æˆ–'skipgram'
    :return: è®­ç»ƒå¥½çš„Word2Vecæ¨¡å‹
    """
    # sg=0è¡¨ç¤ºCBOWï¼Œsg=1è¡¨ç¤ºSkip-gram
    sg = 1 if model_type == 'skipgram' else 0
    model = Word2Vec(
        sentences=corpus,
        vector_size=100,  # è¯å‘é‡ç»´åº¦
        window=5,  # ä¸Šä¸‹æ–‡çª—å£å¤§å°
        min_count=5,  # è¿‡æ»¤ä½é¢‘è¯ï¼ˆæå‡æ¨¡å‹è´¨é‡ï¼‰
        workers=4,  # å¹¶è¡Œçº¿ç¨‹æ•°
        sg=sg,  # æ¨¡å‹æ¨¡å¼
        epochs=10  # è®­ç»ƒè½®æ•°
    )
    print(f"âœ… {model_type.upper()}æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°ï¼š{len(model.wv)}")
    return model


# -------------------------- 3. Word2Vecæ¶æ„å›¾ç”Ÿæˆï¼ˆå¯è§†åŒ–åŸç†ï¼‰ --------------------------
def plot_word2vec_architecture(model_type='cbow'):
    """
    ç»˜åˆ¶Word2Vecä¸¤ç§æ¨¡å¼çš„ç¥ç»ç½‘ç»œæ¶æ„å›¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
    :param model_type: æ¨¡å‹ç±»å‹ï¼Œ'cbow'æˆ–'skipgram'
    """
    plt.figure(figsize=(10, 6))
    plt.rcParams['font.sans-serif'] = ['SimHei']  # æ”¯æŒä¸­æ–‡
    plt.title(f"Word2Vec-{model_type.upper()} ç¥ç»ç½‘ç»œæ¶æ„", fontsize=14)

    # å®šä¹‰å›¾å±‚ä½ç½®
    layers = {
        'è¾“å…¥å±‚': 0.8,
        'éšè—å±‚': 0.5,
        'è¾“å‡ºå±‚': 0.2
    }

    # CBOWï¼šè¾“å…¥ä¸Šä¸‹æ–‡è¯â†’éšè—å±‚â†’è¾“å‡ºç›®æ ‡è¯
    if model_type == 'cbow':
        # è¾“å…¥å±‚ï¼ˆå¤šä¸ªä¸Šä¸‹æ–‡è¯ï¼‰
        for i in range(4):  # ç¤ºä¾‹ï¼š4ä¸ªä¸Šä¸‹æ–‡è¯
            plt.scatter(0.2, layers['è¾“å…¥å±‚'] - i * 0.1, s=200, c='lightblue', label='ä¸Šä¸‹æ–‡è¯' if i == 0 else "")
        # éšè—å±‚ï¼ˆè¯å‘é‡ï¼‰
        plt.scatter(0.5, layers['éšè—å±‚'], s=300, c='orange', label='éšè—å±‚ï¼ˆè¯å‘é‡ï¼‰')
        # è¾“å‡ºå±‚ï¼ˆç›®æ ‡è¯ï¼‰
        plt.scatter(0.8, layers['è¾“å‡ºå±‚'], s=200, c='lightgreen', label='ç›®æ ‡è¯')

    # Skip-gramï¼šè¾“å…¥ç›®æ ‡è¯â†’éšè—å±‚â†’è¾“å‡ºä¸Šä¸‹æ–‡è¯
    else:
        # è¾“å…¥å±‚ï¼ˆç›®æ ‡è¯ï¼‰
        plt.scatter(0.2, layers['è¾“å…¥å±‚'], s=200, c='lightblue', label='ç›®æ ‡è¯')
        # éšè—å±‚ï¼ˆè¯å‘é‡ï¼‰
        plt.scatter(0.5, layers['éšè—å±‚'], s=300, c='orange', label='éšè—å±‚ï¼ˆè¯å‘é‡ï¼‰')
        # è¾“å‡ºå±‚ï¼ˆå¤šä¸ªä¸Šä¸‹æ–‡è¯ï¼‰
        for i in range(4):
            plt.scatter(0.8, layers['è¾“å‡ºå±‚'] - i * 0.1, s=200, c='lightgreen', label='ä¸Šä¸‹æ–‡è¯' if i == 0 else "")

    # ç»˜åˆ¶è¿æ¥çº¿
    plt.axvline(x=0.2, ymin=0.1, ymax=0.9, c='gray', linestyle='--')
    plt.axvline(x=0.5, ymin=0.1, ymax=0.9, c='gray', linestyle='--')
    plt.axvline(x=0.8, ymin=0.1, ymax=0.9, c='gray', linestyle='--')

    plt.xlim(0, 1)
    plt.ylim(0, 1)
    plt.legend(loc='upper right')
    plt.axis('off')
    plt.tight_layout()
    plt.savefig(f"word2vec_{model_type}_architecture.png", dpi=300, bbox_inches='tight')
    print(f"ğŸ“Œ Word2Vec-{model_type.upper()}æ¶æ„å›¾å·²ä¿å­˜ä¸ºword2vec_{model_type}_architecture.png")


# -------------------------- 4. å‘é‡ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆå®éªŒä¸‰è¦æ±‚ï¼‰ --------------------------
def calculate_vector_similarity(model, word1, word2=None, top_n=10):
    """
    è®¡ç®—è¯å‘é‡ç›¸ä¼¼åº¦ï¼šå•ä¸ªè¯çš„ç›¸ä¼¼è¯/ä¸¤ä¸ªè¯çš„ä½™å¼¦ç›¸ä¼¼åº¦
    :param model: è®­ç»ƒå¥½çš„Word2Vecæ¨¡å‹
    :param word1: ç›®æ ‡è¯1
    :param word2: ç›®æ ‡è¯2ï¼ˆå¯é€‰ï¼Œè‹¥ä¸ºNoneåˆ™è¿”å›word1çš„ç›¸ä¼¼è¯ï¼‰
    :param top_n: è¿”å›ç›¸ä¼¼è¯çš„æ•°é‡
    :return: ç›¸ä¼¼åº¦ç»“æœ
    """
    if word1 not in model.wv:
        return f"âŒ è¯æ±‡'{word1}'ä¸åœ¨è¯æ±‡è¡¨ä¸­"

    # è®¡ç®—ä¸¤ä¸ªè¯çš„ä½™å¼¦ç›¸ä¼¼åº¦
    if word2 is not None:
        if word2 not in model.wv:
            return f"âŒ è¯æ±‡'{word2}'ä¸åœ¨è¯æ±‡è¡¨ä¸­"
        similarity = cosine_similarity([model.wv[word1]], [model.wv[word2]])[0][0]
        return f"ğŸ“Š '{word1}'ä¸'{word2}'çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼š{similarity:.4f}"

    # è¿”å›ç›¸ä¼¼è¯åˆ—è¡¨
    similar_words = model.wv.most_similar(word1, topn=top_n)
    result = [f"ğŸ“ˆ ä¸'{word1}'æœ€ç›¸ä¼¼çš„{top_n}ä¸ªè¯ï¼š"]
    for word, score in similar_words:
        result.append(f"   {word}: {score:.4f}")
    return "\n".join(result)


# -------------------------- 5. T-SNEå¯è§†åŒ–ï¼ˆå®éªŒå››è¦æ±‚ï¼‰ --------------------------
def tsne_visualization(model, top_n_words=50):
    # é€‰å–é«˜é¢‘è¯
    words = list(model.wv.index_to_key)[:top_n_words]
    vectors = [model.wv[word] for word in words]
    # å°†åˆ—è¡¨è½¬ä¸ºnumpyæ•°ç»„
    vectors = np.array(vectors)  # è¿™ä¸€æ­¥æ˜¯æ ¸å¿ƒï¼

    # T-SNEé™ç»´ï¼ˆ2ç»´ï¼‰
    tsne = TSNE(n_components=2, random_state=42, perplexity=10)
    vectors_tsne = tsne.fit_transform(vectors)

    # ç»˜åˆ¶å¯è§†åŒ–å›¾
    plt.figure(figsize=(12, 8))
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.title("Word2Vecè¯å‘é‡T-SNEå¯è§†åŒ–", fontsize=14)

    for i, word in enumerate(words):
        plt.scatter(vectors_tsne[i, 0], vectors_tsne[i, 1], c='blue', alpha=0.7)
        plt.text(vectors_tsne[i, 0] + 0.1, vectors_tsne[i, 1] + 0.1, word, fontsize=9)

    plt.xlabel("T-SNEç»´åº¦1")
    plt.ylabel("T-SNEç»´åº¦2")
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.savefig("word2vec_tsne_visualization.png", dpi=300, bbox_inches='tight')
    print("ğŸ“Š T-SNEå¯è§†åŒ–å›¾å·²ä¿å­˜ä¸ºword2vec_tsne_visualization.png")


# -------------------------- 6. ä¸»å‡½æ•°ï¼ˆä¸²è”æ‰€æœ‰å®éªŒæ­¥éª¤ï¼‰ --------------------------
def main():
    # 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
    try:
        corpus, labels, df = load_and_preprocess_data('train_part_1.csv')
        print(f"ğŸ“š æ•°æ®åŠ è½½å®Œæˆï¼Œè¯­æ–™åº“æ ·æœ¬æ•°ï¼š{len(corpus)}")
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
        return

    # 2. è®­ç»ƒCBOWå’ŒSkip-gramä¸¤ç§æ¨¡å‹
    cbow_model = train_word2vec_model(corpus, model_type='cbow')
    skipgram_model = train_word2vec_model(corpus, model_type='skipgram')

    # 3. ç»˜åˆ¶Word2Vecæ¶æ„å›¾
    plot_word2vec_architecture(model_type='cbow')
    plot_word2vec_architecture(model_type='skipgram')

    # 4. è®¡ç®—è¯å‘é‡ç›¸ä¼¼åº¦ï¼ˆç¤ºä¾‹ï¼‰
    print("\n" + "-" * 50)
    print(calculate_vector_similarity(cbow_model, "great"))
    print(calculate_vector_similarity(cbow_model, "great", "excellent"))

    # 5. T-SNEå¯è§†åŒ–
    tsne_visualization(cbow_model, top_n_words=50)

    # 6. ä¿å­˜æ¨¡å‹
    cbow_model.save("word2vec_cbow_model.model")
    skipgram_model.save("word2vec_skipgram_model.model")
    print("\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜ï¼Œå®éªŒä¸€è‡³å®éªŒå››æ ¸å¿ƒè¦æ±‚å®Œæˆï¼")


if __name__ == "__main__":
    main()