import torch
import os
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from collections import Counter
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report

# 1. å°è¯•å¯¼å…¥ Config å’Œ TextCNN
try:
    from main import Config, TextCNN
except ImportError as e:
    print("âŒ å¯¼å…¥é”™è¯¯: æ— æ³•ä» main.py å¯¼å…¥ Config æˆ– TextCNNã€‚")
    exit()

# ==========================================
# 2. æœ¬åœ°é‡æ–°å®ç°æ•°æ®å¤„ç† (é¿å…ä¾èµ– main.py å‡½æ•°)
# ==========================================

# ç¡®ä¿ nltk èµ„æºå­˜åœ¨
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt')
    nltk.download('punkt_tab')


def preprocess_text(text):
    text = str(text).lower()
    return word_tokenize(text)


def build_vocab(texts, max_vocab_size=50000):
    print("æ­£åœ¨ç»Ÿè®¡è¯é¢‘...")
    counter = Counter()
    for text in texts:
        tokens = preprocess_text(text)
        counter.update(tokens)

    vocab = {"<PAD>": 0, "<UNK>": 1}
    most_common = counter.most_common(max_vocab_size - 2)
    for word, _ in most_common:
        vocab[word] = len(vocab)
    return vocab


class TextCNNDataset(Dataset):
    def __init__(self, texts, labels, vocab, max_len):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        tokens = preprocess_text(text)
        unk_idx = self.vocab.get("<UNK>", 1)
        indices = [self.vocab.get(t, unk_idx) for t in tokens]

        if len(indices) < self.max_len:
            indices += [self.vocab.get("<PAD>", 0)] * (self.max_len - len(indices))
        else:
            indices = indices[:self.max_len]

        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)


def get_data_loader(file_path, vocab, config, shuffle=False):
    df = pd.read_csv(file_path, header=None, names=['label', 'title', 'text'])
    # æ ‡ç­¾æ˜ å°„: å‡è®¾è®­ç»ƒæ—¶ 1->0, 2->1
    df['label'] = df['label'].map({1: 0, 2: 1})
    texts = (df['title'].fillna("") + " " + df['text'].fillna("")).tolist()
    labels = df['label'].tolist()

    # è·å–æœ€å¤§é•¿åº¦ï¼Œå¦‚æœ config æ²¡æœ‰åˆ™é»˜è®¤ 128
    max_len = getattr(config, 'max_seq_length', 128)

    dataset = TextCNNDataset(texts, labels, vocab, max_len)
    return DataLoader(dataset, batch_size=getattr(config, 'batch_size', 64), shuffle=shuffle)


# ==========================================
# 3. ä¸»æµ‹è¯•é€»è¾‘
# ==========================================
def test_model():
    config = Config()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # --- è‡ªåŠ¨å¯»æ‰¾æ¨¡å‹æ–‡ä»¶ ---
    possible_paths = [
        "saved_models/textcnn_best_model.pth",  # ä½ æŠ¥é”™ä¸­æåˆ°çš„åå­—
        "saved_models/sentiment_model.pth",  # é»˜è®¤åå­—
        "sentiment_model.pth",
        config.model_save_path if hasattr(config, 'model_save_path') else "none"
    ]

    model_path = None
    for path in possible_paths:
        if os.path.exists(path):
            model_path = path
            break

    if model_path is None:
        print(f"âŒ é”™è¯¯ï¼šåœ¨ saved_models/ ä¸‹æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ã€‚")
        print(f"è¯·å°†ä½ çš„ .pth æ–‡ä»¶é‡å‘½åä¸º 'sentiment_model.pth' å¹¶æ”¾å…¥ saved_models æ–‡ä»¶å¤¹ã€‚")
        return
    else:
        print(f"ğŸš€ æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")

    # --- æ­¥éª¤ A: é‡å»ºè¯æ±‡è¡¨ ---
    train_file = os.path.join(getattr(config, 'data_dir', 'dataset'), getattr(config, 'train_file', 'train.csv'))
    # å¦‚æœæ‰¾ä¸åˆ° train.csvï¼Œå°è¯•æ‰¾ train_part_1.csv (æ ¹æ®ä½ ä¹‹å‰çš„config)
    if not os.path.exists(train_file):
        train_file = os.path.join("dataset", "train_part_1.csv")

    if not os.path.exists(train_file):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è®­ç»ƒæ–‡ä»¶ {train_file}ï¼Œæ— æ³•é‡å»ºè¯æ±‡è¡¨ã€‚")
        return

    print("æ­£åœ¨è¯»å–è®­ç»ƒé›†é‡å»ºè¯æ±‡è¡¨...")
    train_df = pd.read_csv(train_file, header=None, names=['label', 'title', 'text'])
    train_texts = (train_df['title'].fillna("") + " " + train_df['text'].fillna("")).tolist()

    vocab_size = getattr(config, 'max_vocab_size', 50000)
    vocab = build_vocab(train_texts, max_vocab_size=vocab_size)
    print(f"âœ… è¯æ±‡è¡¨é‡å»ºå®Œæˆï¼Œå¤§å°: {len(vocab)}")

    # --- æ­¥éª¤ B: åˆå§‹åŒ–æ¨¡å‹ (ä¿®å¤å‚æ•°ç¼ºå¤±é—®é¢˜) ---
    print("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")

    # è¿™é‡Œæˆ‘ä»¬æ˜¾å¼ä¼ å…¥å‚æ•°ï¼Œä¸å†ä¾èµ– config å±æ€§
    # è¿™äº›æ˜¯ TextCNN çš„ç»å…¸é»˜è®¤å‚æ•°ï¼Œé€šå¸¸ä½ åœ¨ main.py é‡Œä¹Ÿæ˜¯è¿™ä¹ˆå†™çš„
    embedding_dim = getattr(config, 'embedding_dim', 100)  # è¯å‘é‡ç»´åº¦
    num_filters = getattr(config, 'num_filters', 100)  # å·ç§¯æ ¸æ•°é‡
    filter_sizes = getattr(config, 'filter_sizes', [3, 4, 5])  # å·ç§¯æ ¸å°ºå¯¸
    num_classes = 2
    dropout = getattr(config, 'dropout', 0.5)

    try:
        # âœ… ä¿®å¤ç‚¹ï¼šæŒ‰é¡ºåºä¼ å…¥æ‰€æœ‰å‚æ•°
        model = TextCNN(len(vocab), embedding_dim, num_filters, filter_sizes, num_classes, dropout)
    except TypeError as e:
        print(f"âš ï¸ åˆå§‹åŒ–å°è¯•1å¤±è´¥: {e}")
        try:
            # å¤‡é€‰ï¼šæœ‰äº›å®ç°æŠŠ config æ”¾åœ¨ç¬¬ä¸€ä¸ª
            model = TextCNN(config, len(vocab))
        except TypeError as e2:
            print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å½»åº•å¤±è´¥ã€‚è¯·æ£€æŸ¥ main.py ä¸­ TextCNN çš„ __init__ å®šä¹‰ã€‚")
            print(f"è¯¦æƒ…: {e2}")
            return

    model.to(device)

    # --- æ­¥éª¤ C: åŠ è½½æƒé‡ ---
    print("æ­£åœ¨åŠ è½½æƒé‡...")
    try:
        state_dict = torch.load(model_path, map_location=device)
        model.load_state_dict(state_dict)
        print("âœ… æƒé‡åŠ è½½æˆåŠŸï¼")
    except RuntimeError as e:
        print(f"âŒ æƒé‡åŠ è½½å¤±è´¥ (å°ºå¯¸ä¸åŒ¹é…): {e}")
        print("åŸå› ï¼šé‡å»ºçš„è¯æ±‡è¡¨å¤§å°ä¸è®­ç»ƒæ—¶ä¸åŒï¼Œæˆ–è€…å·ç§¯æ ¸å‚æ•°ä¸ä¸€è‡´ã€‚")
        print("è§£å†³ï¼šè¯·ç¡®ä¿ train_file æŒ‡å‘çš„æ–‡ä»¶ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ã€‚")
        return

    # --- æ­¥éª¤ D: è¯„ä¼° ---
    test_file = os.path.join(getattr(config, 'data_dir', 'dataset'), getattr(config, 'test_file', 'test.csv'))
    if not os.path.exists(test_file):
        print("æ‰¾ä¸åˆ°æµ‹è¯•æ–‡ä»¶ï¼Œè·³è¿‡è¯„ä¼°")
        return

    print("æ­£åœ¨åŠ è½½æµ‹è¯•é›†...")
    test_loader = get_data_loader(test_file, vocab, config, shuffle=False)

    print("\n===== å¼€å§‹è¯„ä¼° =====")
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='weighted')

    print("\n" + "=" * 30)
    print(f"ğŸ“Š TextCNN æµ‹è¯•ç»“æœ")
    print("=" * 30)
    print(f"Accuracy : {acc:.4f}")
    print(f"F1 Score : {f1:.4f}")
    print("\nè¯¦ç»†æŠ¥å‘Š:")
    print(classification_report(all_labels, all_preds, target_names=['è´Ÿé¢', 'æ­£é¢']))


if __name__ == "__main__":
    test_model()