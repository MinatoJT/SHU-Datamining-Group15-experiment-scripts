import torch
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os
from transformers import AutoTokenizer
from torch.utils.data import DataLoader

# å¯¼å…¥ä½ çš„é¡¹ç›®æ¨¡å—
from config import Config
from model import SentimentClassifier
from load_data import DataLoader as DataLoaderClass
from dataset import SentimentDataset
# å¯¼å…¥ evaluate å‡½æ•°
from main import evaluate, set_hf_mirrors


# ==========================================
# æ¨¡å— 1: ç»˜å›¾åŠŸèƒ½ (æ•°æ®æ¥è‡ªä½ çš„æ—¥å¿—)
# ==========================================
def plot_reconstructed_history():
    print("\n[1/3] æ­£åœ¨æ ¹æ®å†å²æ—¥å¿—ç”Ÿæˆè®­ç»ƒå›¾è¡¨...")

    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC", "Microsoft YaHei"]
    plt.rcParams['axes.unicode_minus'] = False
    sns.set(font='SimHei', font_scale=1.2)

    # === æ‰‹åŠ¨å½•å…¥çš„è®­ç»ƒæ•°æ® ===
    epochs = [1, 2, 3, 4, 5]
    train_losses = [0.3554, 0.1298, 0.0295, 0.0060, 0.0007]
    val_losses = [0.2681, 0.5123, 0.4448, 0.6190, 0.8904]
    val_accs = [0.9251, 0.9331, 0.9421, 0.9461, 0.9441]

    # === ç»˜å›¾é€»è¾‘ ===
    plt.figure(figsize=(12, 5))

    # å­å›¾1: æŸå¤±æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, 'b-o', label='è®­ç»ƒé›†æŸå¤± (Train Loss)')
    plt.plot(epochs, val_losses, 'r-s', label='éªŒè¯é›†æŸå¤± (Val Loss)')
    plt.title('è®­ç»ƒä¸éªŒè¯æŸå¤±æ›²çº¿')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # å­å›¾2: å‡†ç¡®ç‡æ›²çº¿
    plt.subplot(1, 2, 2)
    plt.plot(epochs, val_accs, 'g-^', label='éªŒè¯é›†å‡†ç¡®ç‡ (Val Accuracy)')
    plt.title('éªŒè¯é›†å‡†ç¡®ç‡å˜åŒ–')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')

    # æ ‡æ³¨æœ€é«˜ç‚¹
    max_acc = max(val_accs)
    max_epoch = epochs[val_accs.index(max_acc)]
    plt.annotate(f'å³°å€¼: {max_acc:.4f}',
                 xy=(max_epoch, max_acc),
                 xytext=(max_epoch, max_acc - 0.005),
                 arrowprops=dict(facecolor='black', shrink=0.05))

    plt.legend()
    plt.grid(True)
    plt.tight_layout()

    # ä¿å­˜å›¾ç‰‡
    save_path = "training_plots_reconstructed.png"
    plt.savefig(save_path, dpi=300)
    print(f"âœ… å›¾è¡¨å·²ä¿å­˜ä¸º: {save_path}")

    # å°è¯•æ˜¾ç¤ºï¼ˆå¦‚æœåœ¨æ”¯æŒGUIçš„ç¯å¢ƒä¸‹ï¼‰
    try:
        plt.show(block=False)
        plt.pause(1)
        plt.close()
    except:
        pass


# ==========================================
# æ¨¡å— 2: æœ¬åœ°é¢„æµ‹å‡½æ•°
# ==========================================
def predict_local(text, model, tokenizer, device, config):
    model.eval()
    with torch.no_grad():
        encoding = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=config.max_seq_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        input_ids = encoding['input_ids'].to(device)
        attention_mask = encoding['attention_mask'].to(device)

        outputs = model(input_ids, attention_mask)
        _, predictions = torch.max(outputs, dim=1)

    return predictions.item()


# ==========================================
# ä¸»ç¨‹åºå…¥å£
# ==========================================
if __name__ == "__main__":
    # --- æ­¥éª¤ 1: ç”Ÿæˆå›¾ç‰‡ ---
    plot_reconstructed_history()

    # --- æ­¥éª¤ 2: å‡†å¤‡æµ‹è¯•ç¯å¢ƒ ---
    print("\n[2/3] åˆå§‹åŒ–æ¨¡å‹ä¸æµ‹è¯•ç¯å¢ƒ...")
    set_hf_mirrors()
    config = Config()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½æ¨¡å‹æ¶æ„
    model = SentimentClassifier(config.model_name, config.num_classes)

    # åŠ è½½æƒé‡
    if os.path.exists(config.model_save_path):
        print(f"âœ… æ­£åœ¨åŠ è½½ä¿å­˜çš„æ¨¡å‹: {config.model_save_path}")
        state_dict = torch.load(config.model_save_path, map_location=device)
        model.load_state_dict(state_dict)
    else:
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ {config.model_save_path}")
        exit()

    model.to(device)
    model.eval()

    # å‡†å¤‡åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)

    # --- æ­¥éª¤ 3: è¿è¡Œæµ‹è¯•é›†è¯„ä¼° ---
    print("\n===== æ­£åœ¨åŠ è½½æµ‹è¯•é›† =====")
    data_loader = DataLoaderClass(config)
    test_texts, test_labels = data_loader.load_csv(config.test_path)

    test_dataset = SentimentDataset(test_texts, test_labels, tokenizer, config.max_seq_length)
    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)

    print("å¼€å§‹æµ‹è¯•é›†è¯„ä¼°...")
    test_loss, test_acc = evaluate(model, test_loader, device)
    print(f"\nğŸ“Š æµ‹è¯•é›†æœ€ç»ˆå‡†ç¡®ç‡: {test_acc:.4f}")

    # --- æ­¥éª¤ 4: è¿è¡Œæ ·ä¾‹é¢„æµ‹ ---
    print("\n[3/3] è¿è¡Œæ ·ä¾‹é¢„æµ‹")
    examples = [
        "è¿™ä¸ªäº§å“è´¨é‡éå¸¸å¥½ï¼Œæˆ‘å¾ˆæ»¡æ„ï¼",
        "ç‰©æµå¤ªæ…¢äº†ï¼ŒåŒ…è£…ä¹Ÿç ´æŸäº†ï¼Œå·®è¯„ã€‚",
        "è™½ç„¶ä»·æ ¼æœ‰ç‚¹è´µï¼Œä½†æ˜¯ç‰©æœ‰æ‰€å€¼ã€‚",
        "ä¸€èˆ¬èˆ¬å§ï¼Œæ²¡æœ‰æƒ³è±¡ä¸­é‚£ä¹ˆå¥½ã€‚",
        "The quality is amazing, I love it!",
        "Terrible experience, waste of money."
    ]

    print(f"{'æ–‡æœ¬':<40} | {'é¢„æµ‹ç»“æœ'}")
    print("-" * 60)
    for text in examples:
        prediction = predict_local(text, model, tokenizer, device, config)
        label = "æ­£é¢ (Positive)" if prediction == 1 else "è´Ÿé¢ (Negative)"
        print(f"{text:<40} | {label}")

    print("\nâœ… æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼è¯·æŸ¥çœ‹ç”Ÿæˆçš„ 'training_plots_reconstructed.png'")